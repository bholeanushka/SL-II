{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical No. 5\n",
    "- Write a python Program for Bidirectional Associative Memory with two pairs of vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.317816\n",
      "Epoch 2/500, Loss: 0.314275\n",
      "Epoch 3/500, Loss: 0.310797\n",
      "Epoch 4/500, Loss: 0.307390\n",
      "Epoch 5/500, Loss: 0.304065\n",
      "Epoch 6/500, Loss: 0.300830\n",
      "Epoch 7/500, Loss: 0.297692\n",
      "Epoch 8/500, Loss: 0.294659\n",
      "Epoch 9/500, Loss: 0.291736\n",
      "Epoch 10/500, Loss: 0.288928\n",
      "Epoch 11/500, Loss: 0.286238\n",
      "Epoch 12/500, Loss: 0.283670\n",
      "Epoch 13/500, Loss: 0.281224\n",
      "Epoch 14/500, Loss: 0.278902\n",
      "Epoch 15/500, Loss: 0.276702\n",
      "Epoch 16/500, Loss: 0.274623\n",
      "Epoch 17/500, Loss: 0.272664\n",
      "Epoch 18/500, Loss: 0.270822\n",
      "Epoch 19/500, Loss: 0.269093\n",
      "Epoch 20/500, Loss: 0.267474\n",
      "Epoch 21/500, Loss: 0.265960\n",
      "Epoch 22/500, Loss: 0.264547\n",
      "Epoch 23/500, Loss: 0.263231\n",
      "Epoch 24/500, Loss: 0.262006\n",
      "Epoch 25/500, Loss: 0.260868\n",
      "Epoch 26/500, Loss: 0.259811\n",
      "Epoch 27/500, Loss: 0.258831\n",
      "Epoch 28/500, Loss: 0.257924\n",
      "Epoch 29/500, Loss: 0.257083\n",
      "Epoch 30/500, Loss: 0.256306\n",
      "Epoch 31/500, Loss: 0.255588\n",
      "Epoch 32/500, Loss: 0.254924\n",
      "Epoch 33/500, Loss: 0.254311\n",
      "Epoch 34/500, Loss: 0.253745\n",
      "Epoch 35/500, Loss: 0.253222\n",
      "Epoch 36/500, Loss: 0.252740\n",
      "Epoch 37/500, Loss: 0.252294\n",
      "Epoch 38/500, Loss: 0.251883\n",
      "Epoch 39/500, Loss: 0.251503\n",
      "Epoch 40/500, Loss: 0.251151\n",
      "Epoch 41/500, Loss: 0.250827\n",
      "Epoch 42/500, Loss: 0.250527\n",
      "Epoch 43/500, Loss: 0.250249\n",
      "Epoch 44/500, Loss: 0.249992\n",
      "Epoch 45/500, Loss: 0.249754\n",
      "Epoch 46/500, Loss: 0.249533\n",
      "Epoch 47/500, Loss: 0.249328\n",
      "Epoch 48/500, Loss: 0.249137\n",
      "Epoch 49/500, Loss: 0.248960\n",
      "Epoch 50/500, Loss: 0.248795\n",
      "Epoch 51/500, Loss: 0.248641\n",
      "Epoch 52/500, Loss: 0.248497\n",
      "Epoch 53/500, Loss: 0.248362\n",
      "Epoch 54/500, Loss: 0.248236\n",
      "Epoch 55/500, Loss: 0.248118\n",
      "Epoch 56/500, Loss: 0.248007\n",
      "Epoch 57/500, Loss: 0.247902\n",
      "Epoch 58/500, Loss: 0.247804\n",
      "Epoch 59/500, Loss: 0.247710\n",
      "Epoch 60/500, Loss: 0.247622\n",
      "Epoch 61/500, Loss: 0.247538\n",
      "Epoch 62/500, Loss: 0.247458\n",
      "Epoch 63/500, Loss: 0.247383\n",
      "Epoch 64/500, Loss: 0.247310\n",
      "Epoch 65/500, Loss: 0.247241\n",
      "Epoch 66/500, Loss: 0.247175\n",
      "Epoch 67/500, Loss: 0.247111\n",
      "Epoch 68/500, Loss: 0.247050\n",
      "Epoch 69/500, Loss: 0.246991\n",
      "Epoch 70/500, Loss: 0.246934\n",
      "Epoch 71/500, Loss: 0.246879\n",
      "Epoch 72/500, Loss: 0.246826\n",
      "Epoch 73/500, Loss: 0.246774\n",
      "Epoch 74/500, Loss: 0.246724\n",
      "Epoch 75/500, Loss: 0.246675\n",
      "Epoch 76/500, Loss: 0.246627\n",
      "Epoch 77/500, Loss: 0.246580\n",
      "Epoch 78/500, Loss: 0.246534\n",
      "Epoch 79/500, Loss: 0.246490\n",
      "Epoch 80/500, Loss: 0.246446\n",
      "Epoch 81/500, Loss: 0.246403\n",
      "Epoch 82/500, Loss: 0.246361\n",
      "Epoch 83/500, Loss: 0.246319\n",
      "Epoch 84/500, Loss: 0.246278\n",
      "Epoch 85/500, Loss: 0.246237\n",
      "Epoch 86/500, Loss: 0.246198\n",
      "Epoch 87/500, Loss: 0.246158\n",
      "Epoch 88/500, Loss: 0.246119\n",
      "Epoch 89/500, Loss: 0.246081\n",
      "Epoch 90/500, Loss: 0.246043\n",
      "Epoch 91/500, Loss: 0.246005\n",
      "Epoch 92/500, Loss: 0.245967\n",
      "Epoch 93/500, Loss: 0.245930\n",
      "Epoch 94/500, Loss: 0.245893\n",
      "Epoch 95/500, Loss: 0.245857\n",
      "Epoch 96/500, Loss: 0.245821\n",
      "Epoch 97/500, Loss: 0.245784\n",
      "Epoch 98/500, Loss: 0.245749\n",
      "Epoch 99/500, Loss: 0.245713\n",
      "Epoch 100/500, Loss: 0.245678\n",
      "Epoch 101/500, Loss: 0.245642\n",
      "Epoch 102/500, Loss: 0.245607\n",
      "Epoch 103/500, Loss: 0.245572\n",
      "Epoch 104/500, Loss: 0.245538\n",
      "Epoch 105/500, Loss: 0.245503\n",
      "Epoch 106/500, Loss: 0.245469\n",
      "Epoch 107/500, Loss: 0.245434\n",
      "Epoch 108/500, Loss: 0.245400\n",
      "Epoch 109/500, Loss: 0.245366\n",
      "Epoch 110/500, Loss: 0.245332\n",
      "Epoch 111/500, Loss: 0.245298\n",
      "Epoch 112/500, Loss: 0.245265\n",
      "Epoch 113/500, Loss: 0.245231\n",
      "Epoch 114/500, Loss: 0.245198\n",
      "Epoch 115/500, Loss: 0.245164\n",
      "Epoch 116/500, Loss: 0.245131\n",
      "Epoch 117/500, Loss: 0.245098\n",
      "Epoch 118/500, Loss: 0.245065\n",
      "Epoch 119/500, Loss: 0.245032\n",
      "Epoch 120/500, Loss: 0.244999\n",
      "Epoch 121/500, Loss: 0.244966\n",
      "Epoch 122/500, Loss: 0.244933\n",
      "Epoch 123/500, Loss: 0.244900\n",
      "Epoch 124/500, Loss: 0.244868\n",
      "Epoch 125/500, Loss: 0.244835\n",
      "Epoch 126/500, Loss: 0.244802\n",
      "Epoch 127/500, Loss: 0.244770\n",
      "Epoch 128/500, Loss: 0.244738\n",
      "Epoch 129/500, Loss: 0.244705\n",
      "Epoch 130/500, Loss: 0.244673\n",
      "Epoch 131/500, Loss: 0.244641\n",
      "Epoch 132/500, Loss: 0.244609\n",
      "Epoch 133/500, Loss: 0.244577\n",
      "Epoch 134/500, Loss: 0.244544\n",
      "Epoch 135/500, Loss: 0.244513\n",
      "Epoch 136/500, Loss: 0.244481\n",
      "Epoch 137/500, Loss: 0.244449\n",
      "Epoch 138/500, Loss: 0.244417\n",
      "Epoch 139/500, Loss: 0.244385\n",
      "Epoch 140/500, Loss: 0.244353\n",
      "Epoch 141/500, Loss: 0.244322\n",
      "Epoch 142/500, Loss: 0.244290\n",
      "Epoch 143/500, Loss: 0.244259\n",
      "Epoch 144/500, Loss: 0.244227\n",
      "Epoch 145/500, Loss: 0.244196\n",
      "Epoch 146/500, Loss: 0.244164\n",
      "Epoch 147/500, Loss: 0.244133\n",
      "Epoch 148/500, Loss: 0.244101\n",
      "Epoch 149/500, Loss: 0.244070\n",
      "Epoch 150/500, Loss: 0.244039\n",
      "Epoch 151/500, Loss: 0.244008\n",
      "Epoch 152/500, Loss: 0.243977\n",
      "Epoch 153/500, Loss: 0.243945\n",
      "Epoch 154/500, Loss: 0.243914\n",
      "Epoch 155/500, Loss: 0.243883\n",
      "Epoch 156/500, Loss: 0.243852\n",
      "Epoch 157/500, Loss: 0.243821\n",
      "Epoch 158/500, Loss: 0.243790\n",
      "Epoch 159/500, Loss: 0.243759\n",
      "Epoch 160/500, Loss: 0.243729\n",
      "Epoch 161/500, Loss: 0.243698\n",
      "Epoch 162/500, Loss: 0.243667\n",
      "Epoch 163/500, Loss: 0.243636\n",
      "Epoch 164/500, Loss: 0.243605\n",
      "Epoch 165/500, Loss: 0.243575\n",
      "Epoch 166/500, Loss: 0.243544\n",
      "Epoch 167/500, Loss: 0.243513\n",
      "Epoch 168/500, Loss: 0.243483\n",
      "Epoch 169/500, Loss: 0.243452\n",
      "Epoch 170/500, Loss: 0.243422\n",
      "Epoch 171/500, Loss: 0.243391\n",
      "Epoch 172/500, Loss: 0.243360\n",
      "Epoch 173/500, Loss: 0.243330\n",
      "Epoch 174/500, Loss: 0.243300\n",
      "Epoch 175/500, Loss: 0.243269\n",
      "Epoch 176/500, Loss: 0.243239\n",
      "Epoch 177/500, Loss: 0.243208\n",
      "Epoch 178/500, Loss: 0.243178\n",
      "Epoch 179/500, Loss: 0.243148\n",
      "Epoch 180/500, Loss: 0.243117\n",
      "Epoch 181/500, Loss: 0.243087\n",
      "Epoch 182/500, Loss: 0.243057\n",
      "Epoch 183/500, Loss: 0.243026\n",
      "Epoch 184/500, Loss: 0.242996\n",
      "Epoch 185/500, Loss: 0.242966\n",
      "Epoch 186/500, Loss: 0.242936\n",
      "Epoch 187/500, Loss: 0.242906\n",
      "Epoch 188/500, Loss: 0.242875\n",
      "Epoch 189/500, Loss: 0.242845\n",
      "Epoch 190/500, Loss: 0.242815\n",
      "Epoch 191/500, Loss: 0.242785\n",
      "Epoch 192/500, Loss: 0.242755\n",
      "Epoch 193/500, Loss: 0.242725\n",
      "Epoch 194/500, Loss: 0.242695\n",
      "Epoch 195/500, Loss: 0.242665\n",
      "Epoch 196/500, Loss: 0.242634\n",
      "Epoch 197/500, Loss: 0.242604\n",
      "Epoch 198/500, Loss: 0.242574\n",
      "Epoch 199/500, Loss: 0.242544\n",
      "Epoch 200/500, Loss: 0.242514\n",
      "Epoch 201/500, Loss: 0.242484\n",
      "Epoch 202/500, Loss: 0.242454\n",
      "Epoch 203/500, Loss: 0.242424\n",
      "Epoch 204/500, Loss: 0.242394\n",
      "Epoch 205/500, Loss: 0.242364\n",
      "Epoch 206/500, Loss: 0.242334\n",
      "Epoch 207/500, Loss: 0.242304\n",
      "Epoch 208/500, Loss: 0.242275\n",
      "Epoch 209/500, Loss: 0.242245\n",
      "Epoch 210/500, Loss: 0.242215\n",
      "Epoch 211/500, Loss: 0.242185\n",
      "Epoch 212/500, Loss: 0.242155\n",
      "Epoch 213/500, Loss: 0.242125\n",
      "Epoch 214/500, Loss: 0.242095\n",
      "Epoch 215/500, Loss: 0.242065\n",
      "Epoch 216/500, Loss: 0.242035\n",
      "Epoch 217/500, Loss: 0.242005\n",
      "Epoch 218/500, Loss: 0.241975\n",
      "Epoch 219/500, Loss: 0.241945\n",
      "Epoch 220/500, Loss: 0.241915\n",
      "Epoch 221/500, Loss: 0.241885\n",
      "Epoch 222/500, Loss: 0.241856\n",
      "Epoch 223/500, Loss: 0.241826\n",
      "Epoch 224/500, Loss: 0.241796\n",
      "Epoch 225/500, Loss: 0.241766\n",
      "Epoch 226/500, Loss: 0.241736\n",
      "Epoch 227/500, Loss: 0.241706\n",
      "Epoch 228/500, Loss: 0.241676\n",
      "Epoch 229/500, Loss: 0.241646\n",
      "Epoch 230/500, Loss: 0.241616\n",
      "Epoch 231/500, Loss: 0.241586\n",
      "Epoch 232/500, Loss: 0.241556\n",
      "Epoch 233/500, Loss: 0.241526\n",
      "Epoch 234/500, Loss: 0.241496\n",
      "Epoch 235/500, Loss: 0.241466\n",
      "Epoch 236/500, Loss: 0.241436\n",
      "Epoch 237/500, Loss: 0.241406\n",
      "Epoch 238/500, Loss: 0.241376\n",
      "Epoch 239/500, Loss: 0.241346\n",
      "Epoch 240/500, Loss: 0.241316\n",
      "Epoch 241/500, Loss: 0.241286\n",
      "Epoch 242/500, Loss: 0.241256\n",
      "Epoch 243/500, Loss: 0.241226\n",
      "Epoch 244/500, Loss: 0.241196\n",
      "Epoch 245/500, Loss: 0.241166\n",
      "Epoch 246/500, Loss: 0.241136\n",
      "Epoch 247/500, Loss: 0.241106\n",
      "Epoch 248/500, Loss: 0.241075\n",
      "Epoch 249/500, Loss: 0.241045\n",
      "Epoch 250/500, Loss: 0.241015\n",
      "Epoch 251/500, Loss: 0.240985\n",
      "Epoch 252/500, Loss: 0.240955\n",
      "Epoch 253/500, Loss: 0.240925\n",
      "Epoch 254/500, Loss: 0.240894\n",
      "Epoch 255/500, Loss: 0.240864\n",
      "Epoch 256/500, Loss: 0.240834\n",
      "Epoch 257/500, Loss: 0.240804\n",
      "Epoch 258/500, Loss: 0.240773\n",
      "Epoch 259/500, Loss: 0.240743\n",
      "Epoch 260/500, Loss: 0.240713\n",
      "Epoch 261/500, Loss: 0.240682\n",
      "Epoch 262/500, Loss: 0.240652\n",
      "Epoch 263/500, Loss: 0.240622\n",
      "Epoch 264/500, Loss: 0.240591\n",
      "Epoch 265/500, Loss: 0.240561\n",
      "Epoch 266/500, Loss: 0.240530\n",
      "Epoch 267/500, Loss: 0.240500\n",
      "Epoch 268/500, Loss: 0.240469\n",
      "Epoch 269/500, Loss: 0.240439\n",
      "Epoch 270/500, Loss: 0.240408\n",
      "Epoch 271/500, Loss: 0.240378\n",
      "Epoch 272/500, Loss: 0.240347\n",
      "Epoch 273/500, Loss: 0.240317\n",
      "Epoch 274/500, Loss: 0.240286\n",
      "Epoch 275/500, Loss: 0.240255\n",
      "Epoch 276/500, Loss: 0.240225\n",
      "Epoch 277/500, Loss: 0.240194\n",
      "Epoch 278/500, Loss: 0.240163\n",
      "Epoch 279/500, Loss: 0.240133\n",
      "Epoch 280/500, Loss: 0.240102\n",
      "Epoch 281/500, Loss: 0.240071\n",
      "Epoch 282/500, Loss: 0.240040\n",
      "Epoch 283/500, Loss: 0.240009\n",
      "Epoch 284/500, Loss: 0.239979\n",
      "Epoch 285/500, Loss: 0.239948\n",
      "Epoch 286/500, Loss: 0.239917\n",
      "Epoch 287/500, Loss: 0.239886\n",
      "Epoch 288/500, Loss: 0.239855\n",
      "Epoch 289/500, Loss: 0.239824\n",
      "Epoch 290/500, Loss: 0.239793\n",
      "Epoch 291/500, Loss: 0.239762\n",
      "Epoch 292/500, Loss: 0.239731\n",
      "Epoch 293/500, Loss: 0.239700\n",
      "Epoch 294/500, Loss: 0.239668\n",
      "Epoch 295/500, Loss: 0.239637\n",
      "Epoch 296/500, Loss: 0.239606\n",
      "Epoch 297/500, Loss: 0.239575\n",
      "Epoch 298/500, Loss: 0.239543\n",
      "Epoch 299/500, Loss: 0.239512\n",
      "Epoch 300/500, Loss: 0.239481\n",
      "Epoch 301/500, Loss: 0.239449\n",
      "Epoch 302/500, Loss: 0.239418\n",
      "Epoch 303/500, Loss: 0.239387\n",
      "Epoch 304/500, Loss: 0.239355\n",
      "Epoch 305/500, Loss: 0.239324\n",
      "Epoch 306/500, Loss: 0.239292\n",
      "Epoch 307/500, Loss: 0.239260\n",
      "Epoch 308/500, Loss: 0.239229\n",
      "Epoch 309/500, Loss: 0.239197\n",
      "Epoch 310/500, Loss: 0.239165\n",
      "Epoch 311/500, Loss: 0.239134\n",
      "Epoch 312/500, Loss: 0.239102\n",
      "Epoch 313/500, Loss: 0.239070\n",
      "Epoch 314/500, Loss: 0.239038\n",
      "Epoch 315/500, Loss: 0.239007\n",
      "Epoch 316/500, Loss: 0.238975\n",
      "Epoch 317/500, Loss: 0.238943\n",
      "Epoch 318/500, Loss: 0.238911\n",
      "Epoch 319/500, Loss: 0.238879\n",
      "Epoch 320/500, Loss: 0.238847\n",
      "Epoch 321/500, Loss: 0.238815\n",
      "Epoch 322/500, Loss: 0.238782\n",
      "Epoch 323/500, Loss: 0.238750\n",
      "Epoch 324/500, Loss: 0.238718\n",
      "Epoch 325/500, Loss: 0.238686\n",
      "Epoch 326/500, Loss: 0.238653\n",
      "Epoch 327/500, Loss: 0.238621\n",
      "Epoch 328/500, Loss: 0.238589\n",
      "Epoch 329/500, Loss: 0.238556\n",
      "Epoch 330/500, Loss: 0.238524\n",
      "Epoch 331/500, Loss: 0.238491\n",
      "Epoch 332/500, Loss: 0.238459\n",
      "Epoch 333/500, Loss: 0.238426\n",
      "Epoch 334/500, Loss: 0.238394\n",
      "Epoch 335/500, Loss: 0.238361\n",
      "Epoch 336/500, Loss: 0.238328\n",
      "Epoch 337/500, Loss: 0.238295\n",
      "Epoch 338/500, Loss: 0.238263\n",
      "Epoch 339/500, Loss: 0.238230\n",
      "Epoch 340/500, Loss: 0.238197\n",
      "Epoch 341/500, Loss: 0.238164\n",
      "Epoch 342/500, Loss: 0.238131\n",
      "Epoch 343/500, Loss: 0.238098\n",
      "Epoch 344/500, Loss: 0.238065\n",
      "Epoch 345/500, Loss: 0.238032\n",
      "Epoch 346/500, Loss: 0.237999\n",
      "Epoch 347/500, Loss: 0.237966\n",
      "Epoch 348/500, Loss: 0.237932\n",
      "Epoch 349/500, Loss: 0.237899\n",
      "Epoch 350/500, Loss: 0.237866\n",
      "Epoch 351/500, Loss: 0.237832\n",
      "Epoch 352/500, Loss: 0.237799\n",
      "Epoch 353/500, Loss: 0.237765\n",
      "Epoch 354/500, Loss: 0.237732\n",
      "Epoch 355/500, Loss: 0.237698\n",
      "Epoch 356/500, Loss: 0.237665\n",
      "Epoch 357/500, Loss: 0.237631\n",
      "Epoch 358/500, Loss: 0.237597\n",
      "Epoch 359/500, Loss: 0.237564\n",
      "Epoch 360/500, Loss: 0.237530\n",
      "Epoch 361/500, Loss: 0.237496\n",
      "Epoch 362/500, Loss: 0.237462\n",
      "Epoch 363/500, Loss: 0.237428\n",
      "Epoch 364/500, Loss: 0.237394\n",
      "Epoch 365/500, Loss: 0.237360\n",
      "Epoch 366/500, Loss: 0.237326\n",
      "Epoch 367/500, Loss: 0.237292\n",
      "Epoch 368/500, Loss: 0.237258\n",
      "Epoch 369/500, Loss: 0.237223\n",
      "Epoch 370/500, Loss: 0.237189\n",
      "Epoch 371/500, Loss: 0.237155\n",
      "Epoch 372/500, Loss: 0.237120\n",
      "Epoch 373/500, Loss: 0.237086\n",
      "Epoch 374/500, Loss: 0.237052\n",
      "Epoch 375/500, Loss: 0.237017\n",
      "Epoch 376/500, Loss: 0.236982\n",
      "Epoch 377/500, Loss: 0.236948\n",
      "Epoch 378/500, Loss: 0.236913\n",
      "Epoch 379/500, Loss: 0.236878\n",
      "Epoch 380/500, Loss: 0.236843\n",
      "Epoch 381/500, Loss: 0.236809\n",
      "Epoch 382/500, Loss: 0.236774\n",
      "Epoch 383/500, Loss: 0.236739\n",
      "Epoch 384/500, Loss: 0.236704\n",
      "Epoch 385/500, Loss: 0.236669\n",
      "Epoch 386/500, Loss: 0.236634\n",
      "Epoch 387/500, Loss: 0.236598\n",
      "Epoch 388/500, Loss: 0.236563\n",
      "Epoch 389/500, Loss: 0.236528\n",
      "Epoch 390/500, Loss: 0.236492\n",
      "Epoch 391/500, Loss: 0.236457\n",
      "Epoch 392/500, Loss: 0.236422\n",
      "Epoch 393/500, Loss: 0.236386\n",
      "Epoch 394/500, Loss: 0.236351\n",
      "Epoch 395/500, Loss: 0.236315\n",
      "Epoch 396/500, Loss: 0.236279\n",
      "Epoch 397/500, Loss: 0.236244\n",
      "Epoch 398/500, Loss: 0.236208\n",
      "Epoch 399/500, Loss: 0.236172\n",
      "Epoch 400/500, Loss: 0.236136\n",
      "Epoch 401/500, Loss: 0.236100\n",
      "Epoch 402/500, Loss: 0.236064\n",
      "Epoch 403/500, Loss: 0.236028\n",
      "Epoch 404/500, Loss: 0.235992\n",
      "Epoch 405/500, Loss: 0.235956\n",
      "Epoch 406/500, Loss: 0.235920\n",
      "Epoch 407/500, Loss: 0.235883\n",
      "Epoch 408/500, Loss: 0.235847\n",
      "Epoch 409/500, Loss: 0.235811\n",
      "Epoch 410/500, Loss: 0.235774\n",
      "Epoch 411/500, Loss: 0.235738\n",
      "Epoch 412/500, Loss: 0.235701\n",
      "Epoch 413/500, Loss: 0.235664\n",
      "Epoch 414/500, Loss: 0.235628\n",
      "Epoch 415/500, Loss: 0.235591\n",
      "Epoch 416/500, Loss: 0.235554\n",
      "Epoch 417/500, Loss: 0.235517\n",
      "Epoch 418/500, Loss: 0.235480\n",
      "Epoch 419/500, Loss: 0.235443\n",
      "Epoch 420/500, Loss: 0.235406\n",
      "Epoch 421/500, Loss: 0.235369\n",
      "Epoch 422/500, Loss: 0.235332\n",
      "Epoch 423/500, Loss: 0.235295\n",
      "Epoch 424/500, Loss: 0.235258\n",
      "Epoch 425/500, Loss: 0.235220\n",
      "Epoch 426/500, Loss: 0.235183\n",
      "Epoch 427/500, Loss: 0.235146\n",
      "Epoch 428/500, Loss: 0.235108\n",
      "Epoch 429/500, Loss: 0.235071\n",
      "Epoch 430/500, Loss: 0.235033\n",
      "Epoch 431/500, Loss: 0.234995\n",
      "Epoch 432/500, Loss: 0.234957\n",
      "Epoch 433/500, Loss: 0.234920\n",
      "Epoch 434/500, Loss: 0.234882\n",
      "Epoch 435/500, Loss: 0.234844\n",
      "Epoch 436/500, Loss: 0.234806\n",
      "Epoch 437/500, Loss: 0.234768\n",
      "Epoch 438/500, Loss: 0.234730\n",
      "Epoch 439/500, Loss: 0.234692\n",
      "Epoch 440/500, Loss: 0.234653\n",
      "Epoch 441/500, Loss: 0.234615\n",
      "Epoch 442/500, Loss: 0.234577\n",
      "Epoch 443/500, Loss: 0.234538\n",
      "Epoch 444/500, Loss: 0.234500\n",
      "Epoch 445/500, Loss: 0.234461\n",
      "Epoch 446/500, Loss: 0.234423\n",
      "Epoch 447/500, Loss: 0.234384\n",
      "Epoch 448/500, Loss: 0.234345\n",
      "Epoch 449/500, Loss: 0.234307\n",
      "Epoch 450/500, Loss: 0.234268\n",
      "Epoch 451/500, Loss: 0.234229\n",
      "Epoch 452/500, Loss: 0.234190\n",
      "Epoch 453/500, Loss: 0.234151\n",
      "Epoch 454/500, Loss: 0.234112\n",
      "Epoch 455/500, Loss: 0.234073\n",
      "Epoch 456/500, Loss: 0.234033\n",
      "Epoch 457/500, Loss: 0.233994\n",
      "Epoch 458/500, Loss: 0.233955\n",
      "Epoch 459/500, Loss: 0.233915\n",
      "Epoch 460/500, Loss: 0.233876\n",
      "Epoch 461/500, Loss: 0.233836\n",
      "Epoch 462/500, Loss: 0.233797\n",
      "Epoch 463/500, Loss: 0.233757\n",
      "Epoch 464/500, Loss: 0.233717\n",
      "Epoch 465/500, Loss: 0.233678\n",
      "Epoch 466/500, Loss: 0.233638\n",
      "Epoch 467/500, Loss: 0.233598\n",
      "Epoch 468/500, Loss: 0.233558\n",
      "Epoch 469/500, Loss: 0.233518\n",
      "Epoch 470/500, Loss: 0.233478\n",
      "Epoch 471/500, Loss: 0.233438\n",
      "Epoch 472/500, Loss: 0.233397\n",
      "Epoch 473/500, Loss: 0.233357\n",
      "Epoch 474/500, Loss: 0.233317\n",
      "Epoch 475/500, Loss: 0.233276\n",
      "Epoch 476/500, Loss: 0.233236\n",
      "Epoch 477/500, Loss: 0.233195\n",
      "Epoch 478/500, Loss: 0.233155\n",
      "Epoch 479/500, Loss: 0.233114\n",
      "Epoch 480/500, Loss: 0.233073\n",
      "Epoch 481/500, Loss: 0.233032\n",
      "Epoch 482/500, Loss: 0.232991\n",
      "Epoch 483/500, Loss: 0.232951\n",
      "Epoch 484/500, Loss: 0.232910\n",
      "Epoch 485/500, Loss: 0.232868\n",
      "Epoch 486/500, Loss: 0.232827\n",
      "Epoch 487/500, Loss: 0.232786\n",
      "Epoch 488/500, Loss: 0.232745\n",
      "Epoch 489/500, Loss: 0.232704\n",
      "Epoch 490/500, Loss: 0.232662\n",
      "Epoch 491/500, Loss: 0.232621\n",
      "Epoch 492/500, Loss: 0.232579\n",
      "Epoch 493/500, Loss: 0.232538\n",
      "Epoch 494/500, Loss: 0.232496\n",
      "Epoch 495/500, Loss: 0.232454\n",
      "Epoch 496/500, Loss: 0.232412\n",
      "Epoch 497/500, Loss: 0.232371\n",
      "Epoch 498/500, Loss: 0.232329\n",
      "Epoch 499/500, Loss: 0.232287\n",
      "Epoch 500/500, Loss: 0.232245\n",
      "\n",
      "Testing the trained model:\n",
      "Input: [0 0], Predicted Output: [0.46389622]\n",
      "Input: [0 1], Predicted Output: [0.43727651]\n",
      "Input: [1 0], Predicted Output: [0.59946015]\n",
      "Input: [1 1], Predicted Output: [0.48633384]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.biases_input_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.biases_hidden_output = np.zeros((1, self.output_size))\n",
    " \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    " \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    " \n",
    "    def forward_propagation(self, inputs):\n",
    "        # Forward propagation through the network\n",
    "        self.hidden_sum = np.dot(inputs, self.weights_input_hidden) + self.biases_input_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output\n",
    "        self.output = self.sigmoid(self.output_sum)\n",
    "        return self.output\n",
    " \n",
    "    def backpropagation(self, inputs, targets):\n",
    "        # Backpropagation\n",
    "        output_errors = targets - self.output\n",
    "        output_gradients = output_errors * self.sigmoid_derivative(self.output)\n",
    "        \n",
    "        hidden_errors = np.dot(output_gradients, self.weights_hidden_output.T)\n",
    "        hidden_gradients = hidden_errors * self.sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_gradients) * self.learning_rate\n",
    "        self.biases_hidden_output += np.sum(output_gradients, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_input_hidden += np.dot(inputs.T, hidden_gradients) * self.learning_rate\n",
    "        self.biases_input_hidden += np.sum(hidden_gradients, axis=0, keepdims=True) * self.learning_rate\n",
    " \n",
    "    def train(self, inputs, targets, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            outputs = self.forward_propagation(inputs)\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.backpropagation(inputs, targets)\n",
    "            \n",
    "            # Calculate and print the loss (MSE)\n",
    "            loss = np.mean(np.square(targets - outputs))\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n",
    " \n",
    "# Example training data (XOR)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    " \n",
    "# Example targets for XOR\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    " \n",
    "# Create and train the neural network\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs =500\n",
    " \n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "nn.train(X, y, epochs)\n",
    " \n",
    "# Test the trained model\n",
    "print(\"\\nTesting the trained model:\")\n",
    "for i in range(len(X)):\n",
    "    output = nn.forward_propagation(X[i])\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {output[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
